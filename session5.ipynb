{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e588970-93a4-47bc-87b2-c11b77dddd3e",
   "metadata": {},
   "source": [
    "# <font color='Blue'>Session 5: neural networks</font>\n",
    "\n",
    "Previously:\n",
    "- Reservoir simulation with DARTS \n",
    "- Techno-economics\n",
    "\n",
    "Today, Learning objectives:\n",
    "- train a neural network\n",
    "- deploy proxy  to optimizer\n",
    "\n",
    "Some useful resources about Machine learning:\n",
    "- TensorFlow website, Basic regression: Predict fuel efficiency. url: https://www.tensorflow.org/tutorials/keras/regression#regression_with_a_deep_neural_network_dnn\n",
    "- Deep Learning by Ian Goodfellow and Yoshua Bengio and Aaron Courville. url: https://www.deeplearningbook.org/\n",
    "- Hands-on \n",
    "Machine Learning\n",
    " with Scikit-Learn \r\n",
    "Keras & TensorlowF by Aurélien Géron. url: https://tudelft.on.worldcat.org/search/detail/1124929613?queryString=Hands-on%20Machine%20Learning%20with%20Scikit-Learnlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e6ec2-7394-4259-9e8b-358f4d4de536",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"perm.PNG\" width=\"40%\">\n",
    "<img style=\"float: left;\" src=\"pressure.PNG\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b101749c-f45e-4274-93e9-f4300734b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary python packages\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf # library for machine learning\n",
    "import pandas as pd \n",
    "#import math \n",
    "\n",
    "from scipy import linalg\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9f0820-59dd-4053-af39-9443b3237400",
   "metadata": {},
   "source": [
    "## <font color='Blue'>Classes</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e987b-dfde-4cc9-a1b2-e57aa6c7d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer():\n",
    "    def __init__(self, wells, NN, mu, min_max):\n",
    "        \"\"\" \n",
    "        ENSEMBLE OPTIMIZATION OF A GEOTHERMAL DOUBLET:\n",
    "            control vector     : u [1 x Nu] e.g. well locations, well bhp, rates.\n",
    "            objective function : Levelized Cost of Heat (LCOH) in euro/MWh \n",
    "        \"\"\"\n",
    "\n",
    "        self.NN = NN\n",
    "        self.mu = mu \n",
    "        self.min_max = min_max \n",
    "        \n",
    "        # control vector paramaters \n",
    "        self.Nt = 1                 # number of control time intervals\n",
    "        self.Nw = len(wells)//2     # number of controlled wells\n",
    "        # self.Nw = 1 \n",
    "        self.Nu = self.Nt*self.Nw   # total number of controls\n",
    "        self.wells = wells\n",
    "        \n",
    "        # optimization parameters\n",
    "        self.Np = self.Nu + 0   # number of perturbations (must be greater than Nu)\n",
    "        self.perturb = 1.0      # perturbation magnitude\n",
    "        self.Nc = 0             # correlation length\n",
    "        \n",
    "        # scaling factors \n",
    "        self.su = 1e3\n",
    "        self.Su = self.su * np.eye(self.Nu)  # scaling factor for control vector \n",
    "        self.Sj = 1e0                        # scaling factor for objective function \n",
    "        \n",
    "        self.u = np.zeros(self.Nu)  # control vector\n",
    "        self.a = np.zeros(self.Nu)  # vector of lower bounds\n",
    "        self.b = np.zeros(self.Nu)  # vector of upper bounds\n",
    "        self.ref = 4000 \n",
    "        self.max = 10000            # maximum allowed control rate\n",
    "        self.min = 4000              # minimum allowed control rate\n",
    "        \n",
    "        # self.u[0:self.Nu] = self.ref # control vector \n",
    "        np.random.seed(1)\n",
    "        self.u = self.ref + (self.su*self.perturb)*np.random.randn(self.Nw) \n",
    "        self.a[0:self.Nu] = self.min   # lower bound\n",
    "        self.b[0:self.Nu] = self.max   # upper bound \n",
    "        \n",
    "        # production well\n",
    "        # self.u[3:] = self.ref \n",
    "        # self.a[3:] = self.min\n",
    "        # self.b[3:] = self.max\n",
    "        \n",
    "        # scale control vector \n",
    "        self.a /= self.su\n",
    "        self.u /= self.su\n",
    "        self.b /= self.su\n",
    "                \n",
    "    def calculate_obj(self, x):\n",
    "        \"\"\" calculate objective functions \"\"\"\n",
    "        obj = self.NN.model((x.reshape(1, self.Nu) - mu)/min_max).numpy()[0][0]\n",
    "        return obj\n",
    "         \n",
    "    def covariance(self):\n",
    "        Cw = np.eye(self.Nt)\n",
    "        if self.Nc > 0:\n",
    "            for i in range(self.Nt):\n",
    "                for j in range(i+1, self.Nt, 1):\n",
    "                    Cw[i,j] = np.exp(-(i-j)**2/self.Nc**2)\n",
    "                    Cw[j,i] = Cw[i,j]\n",
    "        C = Cw\n",
    "        for i in range(1, self.Nw, 1):\n",
    "            C = linalg.block_diag(C,Cw)\n",
    "\n",
    "        return C\n",
    "    \n",
    "    def perturbation(self, u):\n",
    "        \"\"\"\n",
    "        Random distributed perturbations for ensemble gradients generate an ensemble \n",
    "        of control vectors based on uncorrelated Gaussian-distributed perturbations.\n",
    "        This functions returns the Nu x Np matrix U containing perturbed controls.\n",
    "        \n",
    "        u         current control vector\n",
    "        a         vector with lower bounds\n",
    "        b         vector with upper bounds\n",
    "        perturb   perturbation magnitude\n",
    "        Np        number of perturbations\n",
    "        \"\"\"\n",
    "        \n",
    "        U = np.zeros((self.Nu, self.Np))\n",
    "        # np.random.seed(1234)\n",
    "        p = np.random.randn(self.Nu, self.Np)\n",
    "        u = u.reshape((self.Nu, 1))\n",
    "        U = np.tile(u,(1, self.Np)) + self.perturb*p\n",
    "        U = self.truncate(U)\n",
    "        \n",
    "        return U\n",
    "    \n",
    "    def truncate(self, U):\n",
    "        \"\"\"\n",
    "        This function truncates controls at the bounds\n",
    "        U         matrix with control vectors\n",
    "        a         vector with lower bounds\n",
    "        b         vector with upper bounds\n",
    "        \"\"\"\n",
    "        \n",
    "        for j in range(U.shape[1]):\n",
    "            ui   = U[:,j]\n",
    "            for i in range(self.Nu):    \n",
    "                if ui[i] < self.a[i]:\n",
    "                    ui[i] = self.a[i]\n",
    "                if ui[i] > self.b[i]:\n",
    "                    ui[i] = self.b[i]\n",
    "                U[:,j] = ui\n",
    "                \n",
    "        return U\n",
    "    \n",
    "    def compute_gradient(self, u, J, U):\n",
    "        \"\"\"\n",
    "        This function returns the approximate gradient g of an objective function with respect to the controls u.\n",
    "        u         current control vector\n",
    "        J         current objective function value\n",
    "        U         ensemble of perturbed controls\n",
    "        Nt        number of control time intervals\n",
    "        Su        scaling matrix for controls\n",
    "        Sj        scaling factor for objective function values\n",
    "        g         gradient estimate\n",
    "        \"\"\"\n",
    "\n",
    "        # evaluate the objective function values for all perturbed control vectors\n",
    "        jo = np.nan*np.ones((self.Np,1))    \n",
    "        for i in range(self.Np): \n",
    "            # use neural network\n",
    "            objval = self.calculate_obj(U[:,i])\n",
    "            jo[i]  = objval/self.Sj\n",
    "\n",
    "        # anomalies and assign to matrix U and vector j\n",
    "        u  = u.reshape((self.Nu, 1))\n",
    "        dU = U - np.tile(u, (1,self.Np)) \n",
    "        dU = np.transpose(dU)\n",
    "        dj = jo - np.tile(J,(self.Np,1))\n",
    "        \n",
    "        # Singular Value Decomposition\n",
    "        X, S, Vh = linalg.svd(dU, full_matrices=False)\n",
    "        S = np.diag(S)\n",
    "        V = Vh.T\n",
    "\n",
    "        E  = np.sum(np.sum(S**2))\n",
    "        E1 = 0\n",
    "        p  = 0\n",
    "        S1 = np.zeros(np.shape(S))\n",
    "        for i in range(np.min(np.shape(S))):\n",
    "            if E1/E < 0.999:\n",
    "                E1 = E1 + S[i,i]**2\n",
    "                if S[i,i] > 0:\n",
    "                    S1[i,i] = 1/S[i,i]\n",
    "                    p = p + 1\n",
    "        \n",
    "        # compute gradient\n",
    "        g = np.dot(np.dot(np.dot(V[:,0:p] , S1[0:p,0:p]) , X[:,0:p].T), dj)\n",
    "            \n",
    "        return g, jo\n",
    "    \n",
    "    def update(self, u, alpha, g):\n",
    "        \"\"\"\n",
    "        Ths function computes a step size and performs a steepest ascent update\n",
    "        u         current control vector\n",
    "        alpha     proposed step size\n",
    "        g         (preconditioned) gradient vector\n",
    "        a         vector with lower bounds\n",
    "        b         vector with upper bounds\n",
    "        \"\"\"\n",
    "        u = u.reshape((self.Nu, 1))\n",
    "        s = g # use the (preconditioned) gradient as the search direction       \n",
    "        dl = self.a.reshape(self.Nu, 1) - u\n",
    "        du = self.b.reshape(self.Nu, 1) - u\n",
    "        \n",
    "        # set outward directed gradient components to 0 when control is on bound    \n",
    "        for i in range(len(s)):\n",
    "            if (du[i]<=0 and s[i]<0) or (dl[i]>=0 and s[i]<0):\n",
    "                s[i] = 0\n",
    "        \n",
    "        # scale s such that the absolute value of the largest element is equal to 1\n",
    "        if max(abs(s)) > 0:\n",
    "            s = s / max(abs(s))\n",
    "        \n",
    "        # maximum steplength such that a - epsilon <= u + alpha * s <= b + epsilon\n",
    "        i = np.where(s != 0)\n",
    "        epsilon = 0.05\n",
    "        alpha1  = np.min(\n",
    "            np.min([abs((dl[i]-epsilon)/s[i]),  abs((du[i]+epsilon)/s[i])])\n",
    "            )\n",
    "        \n",
    "        if not alpha1:\n",
    "            alpha = min([alpha, alpha1])\n",
    "        \n",
    "        # (preconditioned) steepest ascent update\n",
    "        u1 = u + alpha * s\n",
    "                          \n",
    "        # truncate the updated controls at the lower and upper bounds\n",
    "        u1 = self.truncate(u1)\n",
    "        \n",
    "        return u1\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, train_xn, train_y, test_xn, test_y):#, normalize = False):\n",
    "        \"\"\"\n",
    "        Initialize class\n",
    "        -----------------------------------------------------------------------------------------\n",
    "        train_xn : normalized input data for training,\n",
    "            Size = [m x n_in], m = nr. of training samples, n_in = nr. of input features.\n",
    "        train_y  : labels/target of training data,\n",
    "            Size = [m x n_out], m = nr. of training samples, n_out = nr. of input features.\n",
    "        test_xn  : normalized input of test data,\n",
    "            Size = [m x n_in], m = nr. of training samples, n_in = nr. of input features.\n",
    "        test_y   : labels/target of test data, n_out = nr. of input features.\n",
    "        \"\"\"\n",
    "        # Call base class constructor\n",
    "        super().__init__()\n",
    "        \n",
    "        # training data \n",
    "        self.train_x = train_xn \n",
    "        self.train_y = train_y \n",
    "        \n",
    "        # test data \n",
    "        self.test_x = test_xn \n",
    "        self.test_y = test_y\n",
    "        \n",
    "        # # prepare training data \n",
    "        # training_data = tf.random.shuffle(temp, seed=131154, name=None) # shuffle data \n",
    "        # self.train_x = training_data[:,:3] \n",
    "        # self.train_y = training_data[:,3:] \n",
    "\n",
    "        # # prepare validation/test data \n",
    "        # test_data = tf.random.shuffle(temp_test, seed=250398, name=None) # shuffle data\n",
    "        # self.test_x = test_data[:,:3]\n",
    "        # self.test_y = test_data[:,3:]\n",
    "        \n",
    "        # if normalize:\n",
    "        #     self.mu       = tf.reduce_mean(self.train_x, axis = 0) \n",
    "        #     self.min_max  = tf.reduce_max(self.train_x, axis = 0) - tf.reduce_min(self.train_x, axis = 0) \n",
    "        #     self.train_xn = (self.train_x - self.mu) / self.min_max\n",
    "        #     self.test_xn  = (self.test_x - self.mu) / self.min_max\n",
    "        # else:\n",
    "        #     self.train_xn = self.train_x\n",
    "        #     self.test_xn  = self.test_x \n",
    "    \n",
    "    def create_model(self, n_units = 10, nl = 2, act = 'tanh'):\n",
    "        \"\"\"\n",
    "        Function to build neural network architecture. \n",
    "        n_units : number of neurons per hidden layer \n",
    "        nl      : number of hidden layers \n",
    "        act     : activation function \n",
    "        \"\"\"\n",
    "        seed = 11194 # seed so the initial weights of the layers are random but predictable\n",
    "        \n",
    "        self.model = keras.Sequential() \n",
    "\n",
    "        # input layer\n",
    "        self.model.add(keras.layers.Dense(units = self.train_x.shape[1], \n",
    "                                     kernel_initializer = tf.initializers.GlorotNormal(seed=seed),\n",
    "                                     #bias_initializer = 'zeros', \n",
    "                                     activation = 'linear', \n",
    "                                     name = 'input', \n",
    "                                     input_shape = (self.train_x.shape[1],)))   \n",
    "        \n",
    "        # hidden layers \n",
    "        for i in range(nl):\n",
    "            self.model.add(keras.layers.Dense(units = n_units,\n",
    "                                              kernel_initializer = tf.initializers.GlorotNormal(seed=seed),\n",
    "                                              #bias_initializer = 'zeros',\n",
    "                                              activation = act\n",
    "                                              ))\n",
    "            # self.model.add(keras.layers.LeakyReLU())\n",
    "\n",
    "        # output layer \n",
    "        self.model.add(keras.layers.Dense(units = self.train_y.shape[1], \n",
    "                                     kernel_initializer = tf.initializers.GlorotNormal(seed=seed),\n",
    "                                     bias_initializer = 'zeros', \n",
    "                                     activation = 'linear',\n",
    "                                     name = 'output'))\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "    \n",
    "    def load_model(self, loc):\n",
    "        \"\"\"\n",
    "        Function to load pre-existing model \n",
    "        loc : file location and name \n",
    "        \"\"\"\n",
    "        self.model = tf.keras.models.load_model(loc)\n",
    "        return self.model\n",
    "\n",
    "    def save_model(self, loc):\n",
    "        \"\"\"\n",
    "        function to save trained model\n",
    "        loc : file location\n",
    "        \"\"\"\n",
    "        tf.keras.models.save_model(loc)\n",
    "        return 0\n",
    "        \n",
    "    def custom_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Function for defining custom loss functions. Pass to 'custom_loss' in compile model function.\n",
    "        y_true : labels/truth  \n",
    "        y_pred : neural network prediction \n",
    "        \"\"\"\n",
    "        L1 = tf.reduce_mean(tf.math.square(y_pred - y_true), axis = -1)\n",
    "        return L1 \n",
    "         \n",
    "    def compile_model(self, lr0, decay_rate = 1, decay_steps = 1000, custom_metrics = None, custom_loss = 'MSE'):\n",
    "        \"\"\"\n",
    "        Function to define optimizer and, loss function, metrics and compile. \n",
    "        lr0            : initial learning rate \n",
    "        decay_rate     : exponential decay rate\n",
    "        decay_steps    : number of steps !not the same as the number of epochs! ~= nr_epochs / (nr_samples / batch_size)\n",
    "        custom_metrics : user specified performance metric to monitor neural networks training with\n",
    "        cutsom_loss    : custom loss function, default Mean Squared Error \n",
    "        \"\"\"\n",
    "        self.lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            lr0,\n",
    "            decay_steps=decay_steps,\n",
    "            decay_rate=decay_rate,\n",
    "            staircase=False)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.lr_schedule) # stochastic optimizer \n",
    "        self.model.compile(loss = custom_loss,\n",
    "                           optimizer = self.optimizer,\n",
    "                           metrics = custom_metrics) \n",
    "        \n",
    "    def train_model(self, nr_epochs, batch_size = 32, es = None):\n",
    "        \"\"\"\n",
    "        function call for training neural network \n",
    "        nr_epochs  : number of epochs (training iterations) \n",
    "        batch_size : default 32, number of updates per epoch = nr_samples/batch_size \n",
    "        es         : callback for early stopping function, prevents overfitting \n",
    "        \"\"\"\n",
    "        # exponential_decay_fn = self.exponential_decay(lr0 = 1e-2, s = 250)\n",
    "        # self.lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "        history = self.model.fit(self.train_x, \n",
    "                                 self.train_y,\n",
    "                                 validation_data = (self.test_x, self.test_y),\n",
    "                                 #validation_split = 0.1,\n",
    "                                 batch_size = batch_size,\n",
    "                                 epochs = nr_epochs,\n",
    "                                 verbose = 1, \n",
    "                                 callbacks = es,\n",
    "                                 # use_multiprocessing = False\n",
    "                                 )\n",
    "        return history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac9fdb-d370-490d-92e6-acdee14d2512",
   "metadata": {},
   "source": [
    "## <font color='Blue'>Example 1: neural network</font>\n",
    "<img style=\"float: left;\" src=\"geothermal.PNG\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad7af23-b566-4634-b503-150bba8509fd",
   "metadata": {},
   "source": [
    "***Import data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb6944-8a77-4d6d-a72f-f2b986da0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data \n",
    "test_data = pd.read_pickle('session5data/test_data.pkl')\n",
    "\n",
    "flist = []\n",
    "for i in range(25):\n",
    "    flist.append('session5data/darts_time_data%d.pkl'%i)    \n",
    "\n",
    "# load training data\n",
    "dfs = []\n",
    "for file in flist:\n",
    "    df = pd.read_pickle(file)\n",
    "    df['Simulation'] = 'darts_time_data%d.pkl'%i\n",
    "    dfs.append(df)\n",
    "   \n",
    "# compile single dataframe\n",
    "data = pd.concat(dfs)\n",
    "\n",
    "# reset index to avoid duplicate entries\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# check that we have the simulations as expected\n",
    "data['Simulation'].unique()\n",
    "data.columns.tolist()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185daa51-cf1a-418d-ae26-26e7c3700495",
   "metadata": {},
   "outputs": [],
   "source": [
    "well_names = ['PRD1', 'PRD2']\n",
    "plt.figure(figsize = (9,9))\n",
    "plt.subplots_adjust(wspace = 0.5, hspace = 0.5)\n",
    "for i, w in enumerate(well_names):\n",
    "    # temperature \n",
    "    plt.subplot(2,2,1)\n",
    "    plt.grid()\n",
    "    plt.scatter(data['time (a)'], data[w + ' : water rate (m3/day)'], s = 4, alpha = 0.5, label = w + ': train') \n",
    "    plt.plot(test_data['time (a)'], test_data[w + ' : water rate (m3/day)'], '-k', label = w + ': test')\n",
    "    plt.ylabel('water rate (m3/day)')\n",
    "    plt.xlabel('time (a)')\n",
    "    plt.legend(bbox_to_anchor=(0, 0))   \n",
    "    \n",
    "    # BHP\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.grid()\n",
    "    plt.scatter(data['time (a)'], data[w + ' : BHP (bar)'], s = 4, alpha = 0.5, label = w + \": train\")\n",
    "    plt.plot(test_data['time (a)'], test_data[w + ' : BHP (bar)'], '-k', label = w + ': test')\n",
    "    plt.ylabel('BHP (bar)')\n",
    "    plt.xlabel('time (a)')\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    # rates\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.grid()\n",
    "    plt.scatter(data['time (a)'], data[w + ' : temperature (K)'], s = 4, alpha = 0.5, label = w + ': train')\n",
    "    plt.plot(test_data['time (a)'], test_data[w + ' : temperature (K)'], '-k', label = w + ': test')\n",
    "    plt.ylabel('temperature (K)')\n",
    "    plt.xlabel('time (a)')\n",
    "    plt.legend(bbox_to_anchor=(0, 0)) \n",
    "    \n",
    "# lcoh \n",
    "plt.subplot(2,2,4)\n",
    "plt.scatter(data['time (a)'], data['LCOH'], s = 4, alpha = 0.5, label = 'train') \n",
    "plt.plot(test_data['time (a)'], test_data['LCOH'], '-k', label = 'test')\n",
    "plt.ylabel('LCOH (euro/MWh)')\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.xlabel('time (a)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81d6f12-ba23-41d2-8021-574af4b0f39d",
   "metadata": {},
   "source": [
    "***Prepare data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972cc493-24bb-4155-8933-5c0cb71d29cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training data \n",
    "train_x  = np.vstack([\n",
    "    data['time (a)'],\n",
    "    data['INJ1 : water rate (m3/day)'], \n",
    "    data['INJ2 : water rate (m3/day)']\n",
    "    ]).T\n",
    "train_y  = np.vstack([\n",
    "    #data['PRD1 : temperature (K)'], \n",
    "    #data['PRD2 : temperature (K)'],\n",
    "    data['LCOH'], \n",
    "    ]).T \n",
    "\n",
    "# define test data \n",
    "test_x  = np.vstack([\n",
    "    test_data['time (a)'],\n",
    "    test_data['INJ1 : water rate (m3/day)'], \n",
    "    test_data['INJ2 : water rate (m3/day)']\n",
    "    ]).T\n",
    "test_y  = np.vstack([\n",
    "    #test_data['PRD1 : temperature (K)'], \n",
    "    #test_data['PRD2 : temperature (K)'],\n",
    "    test_data['LCOH'], \n",
    "    ]).T \n",
    "\n",
    "# train_y  = np.vstack([\n",
    "#     -(data['PRD1 : energy (kJ/day)'] + data['PRD2 : energy (kJ/day)']) - (data['INJ1 : energy (kJ/day)'] + data['INJ2 : energy (kJ/day)'])\n",
    "#                       ]).T \n",
    "\n",
    "n = train_x.shape[0] # number of input features\n",
    "m = train_x.shape[1] # number of training samples\n",
    "\n",
    "# normalize data \n",
    "mu       = train_x.mean(axis = 0)\n",
    "min_max  = train_x.max(axis = 0) - train_x.min(axis = 0)\n",
    "train_xn = (train_x - mu)/min_max\n",
    "test_xn  = (test_x - mu)/min_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38e792-a4bb-47e4-9fdb-9c90d620ef01",
   "metadata": {},
   "source": [
    "***Construct neural network***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7872809-0aa4-4093-b4a5-2b257b26ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters \n",
    "nl  = 2         # number of hidden layers \n",
    "nu  = 10        # number of neurons per layer\n",
    "act = 'tanh'    # activation function \n",
    "lr0 = 1e-3      # learning rate \n",
    "\n",
    "NN = NeuralNetwork(train_xn, train_y/1e2, test_xn, test_y/1e2) \n",
    "NN.create_model(n_units = nu, nl = nl) \n",
    "NN.model.summary() \n",
    "NN.compile_model(lr0, decay_rate = 1, decay_steps = 32000, custom_metrics = None, custom_loss = 'MSE') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1cf6f-01f9-4cd8-9652-0717d6a29af6",
   "metadata": {},
   "source": [
    "***Train neural network***\n",
    "\n",
    "During training the collection of weights and biases is updated to minimize the loss function. The loss function is defined as the mean squared error the of the neural networks prediction and the corresponding label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d034e-0d12-48cb-aa2f-ab5cd26b2976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "nr_epochs = 250 # number of epochs/training iterations \n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=0\n",
    ")\n",
    "\n",
    "history = NN.train_model(nr_epochs, \n",
    "                         batch_size = batch_size, \n",
    "                         es = None)\n",
    "\n",
    "plt.figure(figsize = (12,4)) \n",
    "plt.subplot(1,2,1)\n",
    "plt.grid()\n",
    "plt.semilogy(history['loss'], label = 'training loss')\n",
    "plt.semilogy(history['val_loss'], label = 'test loss') \n",
    "plt.xlabel('epoch') \n",
    "plt.ylabel('loss - MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658529c-9b89-4c63-a90e-f5c0c5ce1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,2), dpi = 100)\n",
    "plt.subplots_adjust(wspace = 0.4)\n",
    "plt.subplot(1,2,1)\n",
    "plt.grid()\n",
    "plt.title('Training')\n",
    "plt.plot(train_x[:,0], train_y, '.r')\n",
    "plt.plot(train_x[:,0], NN.model(train_xn)*1e2,'.k', alpha = 0.1)\n",
    "plt.xlabel('time (a)')\n",
    "#plt.ylabel('T (K)')\n",
    "plt.subplot(1,2,2)\n",
    "plt.grid()\n",
    "plt.title('Test')\n",
    "plt.plot(test_x[:,0], test_y, '-r')\n",
    "plt.plot(test_x[:,0], NN.model(test_xn)*1e2 ,'--k')\n",
    "plt.xlabel('time (a)')\n",
    "#plt.ylabel('T (K)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,2), dpi = 100)\n",
    "plt.subplots_adjust(wspace = 0.4)\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Training')\n",
    "plt.grid()\n",
    "plt.plot(train_y, NN.model(train_xn)*1e2, '.')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel(r'$y_{\\theta}$')\n",
    "#plt.legend(['PRD1', 'PRD2'])\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Test')\n",
    "plt.grid()\n",
    "plt.plot(test_y, NN.model(test_xn)*1e2, '.')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel(r'$y_{\\theta}$')\n",
    "#plt.legend(['PRD1', 'PRD2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301a2db-608a-4ac9-ba8d-1b06850304e6",
   "metadata": {},
   "source": [
    "## <font color='Blue'>Example 2: optimization</font>\n",
    "\n",
    "In this optimization problem we define the controls as the rates per doublet and the objective function (aka the cost function) as the levelized cost of heat. Here, we assume that the rate is constant for 30years.\n",
    "\n",
    "<img style=\"float: left;\" src=\"geothermal2.PNG\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9220d-d23a-4f51-ac72-034c5216ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training set\n",
    "train_x = np.load('session5data/data_x_model2.npy')*1e3 # m3/day\n",
    "train_y = np.load('session5data/data_y_model2.npy').reshape(-1,1) # levelized cost of heat euro/MWh\n",
    "\n",
    "# import test set\n",
    "test_x = np.load('session5data/u_2.npy')*1e3 # well rates [INJ1, INJ2] ,  m3/day\n",
    "test_y = np.load('session5data/j_2.npy').reshape(-1,1) # levelized cost of heat euro/MWh\n",
    "\n",
    "plt.figure(figsize = (12,4)) \n",
    "plt.subplot(1,2,1)\n",
    "plt.grid()\n",
    "plt.title('Training data')\n",
    "c = plt.scatter(train_x[:,0], train_x[:,1], c = train_y, cmap = 'jet') \n",
    "plt.colorbar(c, label = 'euro/MWh @ year 30')\n",
    "plt.xlabel(r'$q1 [m^3/day]$')\n",
    "plt.ylabel(r'$q2 [m^3/day]$')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Test data') \n",
    "plt.grid()\n",
    "c = plt.scatter(test_x[:,0], test_x[:,1], c = test_y, cmap = 'jet') \n",
    "plt.colorbar(c, label = 'euro/MWh @ year 30')\n",
    "plt.xlabel(r'$q1 [m^3/day]$')\n",
    "plt.ylabel(r'$q2 [m^3/day]$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5cc2f1-9b79-4b31-886b-75a2366ce8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize training data \n",
    "#mu = np.mean(train_x, axis = 0) \n",
    "#min_max = train_x.max(axis = 0) - train_x.min(axis = 0) \n",
    "\n",
    "mu       = 0\n",
    "min_max  = 1e3\n",
    "train_xn = (train_x - mu) / min_max\n",
    "test_xn  = (test_x - mu) / min_max \n",
    "\n",
    "plt.figure(dpi = 100)\n",
    "plt.subplot(2,2,1) \n",
    "plt.hist(train_xn[:,0]) \n",
    "plt.hist(test_xn[:,0])\n",
    "plt.ylabel('x1')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(train_xn[:,1], train_xn[:,0], 'x') \n",
    "plt.plot(test_xn[:,1], test_xn[:,0], 'x') \n",
    "plt.subplot(2,2,3) \n",
    "plt.plot(train_xn[:,0], train_xn[:,1], 'x') \n",
    "plt.plot(test_xn[:,0], test_xn[:,1], 'x')\n",
    "plt.xlabel('x1') \n",
    "plt.ylabel('x2')\n",
    "plt.subplot(2,2,4) \n",
    "plt.hist(train_xn[:,1]) \n",
    "plt.hist(test_xn[:,1])\n",
    "plt.xlabel('x2') \n",
    "plt.legend(['training data', 'test data'])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9a29d-a056-4613-aff7-dfa301368df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl  = 4  # number of hidden layers \n",
    "nu  = 10 # number of neurons per layer \n",
    "act = 'tanh' # activation function \n",
    "lr0 = 1e-3 # learning rate \n",
    "\n",
    "NN = NeuralNetwork(train_xn, train_y/1e2, test_xn, test_y/1e2)\n",
    "NN.create_model(n_units = nu, nl = nl) # create model \n",
    "NN.model.summary() \n",
    "NN.compile_model(lr0, decay_rate = 0.5, decay_steps = 32000, custom_metrics = None, custom_loss = 'MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed5ce81-e2d5-421b-a993-3a18510e93b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train neural network\n",
    "batch_size = 32 \n",
    "nr_epochs = 200 # number of epochs\n",
    "history = NN.train_model(nr_epochs, batch_size = 32, es = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374629e-6753-449b-a9a4-de3e31b8a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (9,4)) \n",
    "plt.subplot(1,2,1)\n",
    "plt.grid()\n",
    "plt.semilogy(history['loss'], label = 'training loss')\n",
    "plt.semilogy(history['val_loss'], label = 'test loss') \n",
    "plt.xlabel('epoch') \n",
    "plt.ylabel('loss - MSE')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.grid()\n",
    "loss_fn = keras.losses.MeanSquaredError() \n",
    "loss_training = loss_fn(train_y, 1e2*NN.model(train_xn))\n",
    "loss_test = loss_fn(test_y, 1e2*NN.model(test_xn))\n",
    "plt.plot(train_y, 1e2 * NN.model(train_xn), 'x', label = 'training data, loss = %.2e'%loss_training)\n",
    "plt.plot(test_y, 1e2 * NN.model(test_xn), 'x', label = 'test data, loss = %.2e'%loss_test)\n",
    "plt.xlabel('y')\n",
    "plt.ylabel(r'$\\hat{y}$')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdad857-cc2b-40ea-85af-c50476169a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (9,3)) \n",
    "plt.subplots_adjust(wspace = 0.4) \n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "mini = 4000\n",
    "maxi = 10000\n",
    "\n",
    "# APPROXIMATE SOLUTION SPACE WITH NEURAL NETWORK\n",
    "x = np.linspace(mini, maxi, 25)\n",
    "X = np.meshgrid(x, x)\n",
    "umesh = np.vstack([X[0].flatten(), X[1].flatten()]).T\n",
    "jmesh = NN.model((umesh-mu)/min_max).numpy()*1e2 \n",
    "plt.title(r'$j_{\\theta}(u)$')\n",
    "c = plt.contourf(x, x, jmesh.reshape(25, 25), cmap = cm.jet)\n",
    "#c = plt.scatter(umesh[:,0], umesh[:,1], c = jmesh, cmap = cm.jet)\n",
    "plt.colorbar(c, label = 'LCOH eur/MWh')\n",
    "plt.xlabel('q1')\n",
    "plt.ylabel('q2')\n",
    "plt.xlim(mini, maxi)\n",
    "plt.ylim(mini, maxi)\n",
    "\n",
    "# TRUTH (DARTS)\n",
    "plt.subplot(1,2,2)\n",
    "umesh_truth = np.load('session5data/meshu.npy')                      \n",
    "jmesh_truth = np.load('session5data/meshj.npy').reshape(-1,1)  \n",
    "plt.title(r'$j_{\\theta}(u)-j(u)$')\n",
    "c = plt.contourf(x, x, (jmesh_truth).reshape(25, 25), cmap = cm.jet)\n",
    "#c = plt.scatter(umesh_truth[:,0], umesh_truth[:,1], c = jmesh_truth - jmesh, cmap = cm.jet)\n",
    "plt.colorbar(c, label = 'eur/MWh')\n",
    "plt.xlabel('q1')\n",
    "plt.ylabel('q2')\n",
    "plt.xlim(mini, maxi)\n",
    "plt.ylim(mini, maxi)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c869087-d7b2-4f9a-96e1-5bc3a43ec79d",
   "metadata": {},
   "source": [
    "## <font color='Blue'>Optimize</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be075c5-51b7-40e1-be06-0cc6508e7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "wells = {\n",
    "    'INJ1': [15, 14, 1], \n",
    "    'INJ2': [45, 46, 1], \n",
    "    'PRD1': [15, 46, 1], \n",
    "    'PRD2': [45, 14, 1]\n",
    "    }\n",
    "\n",
    "data_x = []\n",
    "data_y = []\n",
    "u_evolution = []\n",
    "j_evolution = []\n",
    "\n",
    "o = optimizer(wells, NN, mu, min_max)\n",
    "u = o.u\n",
    "j = o.calculate_obj(u*o.su)\n",
    "print('iter 0 \\t, control vector =', u*o.su, '\\t j =', j*1e2) \n",
    "\n",
    "Ni = 200  # number of outer iterations \n",
    "Nl = 10   # number of line iterations\n",
    "C = o.covariance()\n",
    "alpha0 = 0.1 # initial step size \n",
    "\n",
    "for outer in range(1, Ni+1):\n",
    "    U = o.perturbation(u) # generate ensemble of perturbed controls [Nu x Np]  \n",
    "\n",
    "    # gradient evaluation [Nu x 1]\n",
    "    g, J = o.compute_gradient(u, j, U) # compute objective function with neural network\n",
    "    \n",
    "    for i in range(o.Np):\n",
    "        data_x.append(U[:,i])\n",
    "        data_y.append(J[i,0])\n",
    "    \n",
    "    alpha = alpha0\n",
    "    inner = 0\n",
    "    while inner <= Nl:\n",
    "        un = o.update(u, alpha, -np.matmul(C,g)) # update of controls\n",
    "        un = un.flatten()\n",
    "        jn = o.calculate_obj(un * o.su)\n",
    "        \n",
    "        if jn < j or inner == Nl:\n",
    "            u = un # update control with better value\n",
    "            j = jn \n",
    "            \n",
    "            u_evolution.append(un*o.su)\n",
    "            j_evolution.append(jn*1e2)\n",
    "            \n",
    "            print('iter %d'%outer, '\\t, control vector =', u*o.su, '\\t j =', j*1e2) \n",
    "            break \n",
    "            \n",
    "        else:\n",
    "            alpha *= 0.5 # reduce step size\n",
    "            \n",
    "        inner = inner + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c5c3a-6958-441a-807d-f61431d7b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot evolution of training \n",
    "uevo = np.asarray(u_evolution)\n",
    "jevo = np.asarray(j_evolution)\n",
    "\n",
    "plt.figure()\n",
    "plt.grid()\n",
    "c = plt.contourf(x, x, jmesh.reshape(25, 25), cmap = cm.jet)\n",
    "#plt.scatter(train_x[:,0], train_x[:,1], label = 'training data') \n",
    "plt.scatter(uevo[:,0], uevo[:,1], s = 5, c = 'k')\n",
    "plt.colorbar(c)\n",
    "plt.xlabel('q1')\n",
    "plt.ylabel('q2')\n",
    "plt.legend()\n",
    "plt.xlim(o.min, o.max)\n",
    "plt.ylim(o.min, o.max)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
